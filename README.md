# Dask Executor Scheduler

A Dask scheduler that uses a Python _concurrent.futures.Executor_ to run tasks.

The motivation for building this was as a way to get Dask use serverless cloud functions for executing tasks.
Using serverless cloud functions allows scaling to thousands of concurrent workers, with no cluster to set up and manage.
This code has been used with [Pywren](https://github.com/pywren), see instructions below.

The implementation is fairly naive - tasks are placed on an in-memory queue and processed by the [executor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor) in batches.

All the tasks in a batch must have the same function that is being applied. This is so the executor's _map_ method can be used on the batch. Tasks are accumulated in a batch until they reach a certain size, or there are no more tasks with a matching function, or a timeout occurs - whichever happens first.

The tasks are generated by the Dask local scheduler, so there is no guarantee that they will be produced in an order that works well for this style of execution. However, batch-style parallel processing is generally a good fit for this scheduler.

For testing, it's useful to use a [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor). This is the default if no executor is specified.

Have a look in the examples directory to see how to use the scheduler.

### Installation

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install .
pip install -e ../pywren-ibm-cloud
```

### Running locally

Local thread pool:

```bash
python examples/threadpool_executor.py
```

Pywren using a local executor:

```bash
python examples/pywren_local_executor.py
```

### Configuring Pywren for Google Cloud

I've created a branch of `pywren-ibm-cloud` with support for Google Cloud Storage and Google Cloud Run here: https://github.com/tomwhite/pywren-ibm-cloud.

Edit your _~/.pywren_config_ file as follows, where `<BUCKET>` is the name of a newly-created bucket:

```
pywren:
    storage_bucket: <BUCKET>
    storage_backend: gcsfs
    compute_backend: cloudrun

gcsfs:
    project_id: <PROJECT_ID>

cloudrun:
    project_id: <PROJECT_ID>
    region: <REGION>
```

Run using the Cloud Run executor:

```bash
python examples/pywren_cloudrun_executor.py
```

### Related projects

The idea for this came from the work I did in [Zappy](https://github.com/lasersonlab/zappy) to run NumPy processing on Pywren.
